\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{xeCJK}

\geometry{margin=1in}

\title{CS307 Fall 2025 - Project Part I}
\author{
    李语尚 (12412308) \\
    丁哲昊 (12412310)
}
\date{\today}

\begin{document}

\maketitle

\section{Group Information}

\begin{itemize}
    \item 李语尚 - 12412308
    \item 丁哲昊 - 12412310
\end{itemize}

\section{Task 1: E-R Diagram}

We use boardmix to draw the E-R diagram.

\includegraphics[width=\textwidth]{1.png}

\section{Task 2: Database Design}

\subsection{E-R Diagram by Datagrip}
\includegraphics[width=\textwidth]{2.png}


\subsection{Table Design Description}

In order to model the relationships within the recipe and user ecosystem, and to ensure the database design meets the requirements of data integrity and expandability, we have designed the following tables, categorized into Core Entities, One-to-One/One-to-Many Relations, and Many-to-Many Junctions.

\subsubsection{Core Entity Tables}

\paragraph{users}
\begin{itemize}
    \item \textbf{authorid}: The unique identifier for the user, which serves as the primary key of this table.
    \item \textbf{authorname}: The name of the user.
    \item \textbf{gender}: The gender of the user.
    \item \textbf{age}: The age of the user.
\end{itemize}

\paragraph{recipes}
\begin{itemize}
    \item \textbf{recipeid}: The unique identification number for a recipe, serving as the primary key.
    \item \textbf{authorid}: A foreign key referencing \texttt{users.authorid}, linking the recipe to its creator. The \texttt{ON DELETE SET NULL} constraint ensures the recipe remains even if the user is deleted.
    \item \textbf{name}: The title of the recipe.
    \item \textbf{datepublished}: The timestamp when the recipe was published.
    \item \textbf{recipeservings}: The number of servings the recipe yields.
\end{itemize}

\paragraph{reviews}
\begin{itemize}
    \item \textbf{reviewid}: The unique identifier for a review, which is the primary key.
    \item \textbf{recipeid}: A foreign key referencing \texttt{recipes.recipeid}, indicating which recipe is being reviewed (\texttt{ON DELETE CASCADE}).
    \item \textbf{authorid}: A foreign key referencing \texttt{users.authorid}, indicating which user wrote the review (\texttt{ON DELETE CASCADE}).
    \item \textbf{rating}: The numerical rating given to the recipe (not null).
    \item \textbf{review}: The text content of the review.
\end{itemize}

\subsubsection{One-to-One and One-to-Many Relation Tables}

\paragraph{nutrition}
\begin{itemize}
    \item \textbf{recipeid}: Acts as both the primary key and a foreign key referencing \texttt{recipes.recipeid} (\texttt{ON DELETE CASCADE}), enforcing a one-to-one relationship with the recipes table.
    \item \textbf{calories}: The caloric content (not null).
    \item \textbf{fatcontent, proteincontent, sugarcontent, etc.}: Detailed nutritional information for the specific recipe.
\end{itemize}

\paragraph{instructions}
\begin{itemize}
    \item \textbf{Composite Primary Key}: \texttt{(recipeid, stepnumber)}, uniquely identifying each step for a given recipe.
    \item \textbf{recipeid}: A foreign key referencing \texttt{recipes.recipeid} (\texttt{ON DELETE CASCADE}).
    \item \textbf{stepnumber}: The order of the instruction step (1, 2, 3, \ldots).
    \item \textbf{instructiontext}: The description of the cooking step.
\end{itemize}

\subsubsection{Many-to-Many (M:M) Connection Tables}

\paragraph{ingredients}
\begin{itemize}
    \item \textbf{ingredientid}: A self-increasing primary key.
    \item \textbf{ingredientname}: The name of the ingredient, which must be unique and not null.
\end{itemize}

\paragraph{recipe\_ingredients}
\begin{itemize}
    \item \textbf{Composite Primary Key}: \texttt{(recipeid, ingredientid)}, linking a recipe to an ingredient.
    \item This table resolves the M:M relationship between recipes and ingredients.
\end{itemize}

\paragraph{keywords}
\begin{itemize}
    \item \textbf{keywordid}: A self-increasing primary key.
    \item \textbf{keywordtext}: The text of the keyword, which must be unique and not null.
\end{itemize}

\paragraph{recipe\_keywords}
\begin{itemize}
    \item \textbf{Composite Primary Key}: \texttt{(recipeid, keywordid)}, linking a recipe to a keyword.
    \item This table resolves the M:M relationship between recipes and keywords.
\end{itemize}

\paragraph{user\_favorite\_recipes}
\begin{itemize}
    \item \textbf{Composite Primary Key}: \texttt{(authorid, recipeid)}, recording a user's favorite recipe.
    \item This table resolves the M:M relationship between users and recipes (Favorites).
\end{itemize}

\paragraph{user\_liked\_reviews}
\begin{itemize}
    \item \textbf{Composite Primary Key}: \texttt{(authorid, reviewid)}, recording which user liked which review.
    \item This table resolves the M:M relationship between users and reviews (Likes).
\end{itemize}

\paragraph{user\_follows}
\begin{itemize}
    \item \textbf{Composite Primary Key}: \texttt{(followerid, followingid)}, recording a user following another user (a self-referencing M:M relationship).
    \item \textbf{followerid}: The ID of the user who is following.
    \item \textbf{followingid}: The ID of the user who is being followed.
\end{itemize}

\subsection{Scalability}

The tables we've designed exhibit strong scalability features, primarily due to the use of dedicated junction tables for many-to-many relationships and effective foreign key management:

\paragraph{Table users and recipes (Core Entities):}
\begin{itemize}
    \item The core entities are designed with simple primary keys (\texttt{authorid}, \texttt{recipeid}) and efficient data types (\texttt{bigint}), making lookups and direct access operations fast.
    \item The one-to-many relationship (one user publishes many recipes) is handled efficiently via \texttt{recipes.authorid}. New users and recipes can be added easily without complex table rewriting.
\end{itemize}

\paragraph{Junction Tables (e.g., recipe\_ingredients, user\_follows):}
\begin{itemize}
    \item Tables like \texttt{recipe\_ingredients}, \texttt{recipe\_keywords}, \texttt{user\_favorite\_recipes}, \texttt{user\_liked\_reviews}, and \texttt{user\_follows} are designed with composite primary keys consisting of two \texttt{bigint} foreign keys.
    \item This structure efficiently models M:M relationships, allowing for an indefinite growth in connections (e.g., a recipe can have virtually unlimited ingredients, and a user can follow unlimited other users).
    \item Adding or deleting a new relationship (e.g., a user favoring a recipe) involves a simple insertion or deletion of a row into these junction tables, which is a fast, isolated operation, promoting high insertion throughput.
\end{itemize}

\section{Task 3: Data Import}

\subsection{System Architecture}

\subsubsection{Project Structure}

Our data import system is organized into task-specific directories:

\begin{verbatim}
DataBase/
├── src/main/
│   ├── task3/                       - Task 3: Data Import Module
│   │   ├── Main.java                - Entry point for batch insert method
│   │   ├── MainCompare.java         - Entry point for single insert method
│   │   ├── CsvDataImporter.java     - Batch insert import orchestrator
│   │   ├── CsvDataImporterCompare.java - Single insert import orchestrator
│   │   ├── DataReader.java          - CSV parsing utilities
│   │   ├── DataWriter.java          - Batch insert operations
│   │   ├── DataQuery.java           - Query utilities for validation
│   │   └── database_schema.sql      - Database schema definition (12 tables)
│   ├── task4/                       - Task 4: Performance Testing Module
│   │   ├── PerformanceTest.java     - Main performance test orchestrator
│   │   ├── FileIOOperations.java    - File I/O operations for comparison
│   │   └── BTreeIndex.java         - In-memory B-Tree index implementation
│   └── common/                      - Shared Components
│       ├── ConnectionManager.java   - Database connection and transaction management
│       ├── DatabaseConfig.java      - Configuration loader (from db.properties)
│       └── db.properties            - Database connection configuration
├── final_data/                      - Input CSV Data
│   ├── user.csv                     - User data (299,892 records)
│   ├── recipes.csv                  - Recipe data (928,283 records)
│   └── reviews.csv                  - Review data (1,639,086 records)
├── postgresql-42.2.5.jar            - PostgreSQL JDBC driver
└── Task3_DataImport.zip            - Task 3 source code archive
    Task4_PerformanceTest.zip       - Task 4 source code archive
    Common_SharedFiles.zip          - Shared components archive
\end{verbatim}

\subsubsection{Key Components}

\textbf{1. Main Entry Points:}
\begin{itemize}
    \item \texttt{Main.java}: Orchestrates the batch insert import process
    \item \texttt{MainCompare.java}: Orchestrates the single insert import process for comparison
\end{itemize}

\textbf{2. Import Orchestrators:}
\begin{itemize}
    \item \texttt{CsvDataImporter.java}: Implements optimized batch insert method
    \begin{itemize}
        \item Uses \texttt{PreparedStatement.addBatch()} with batch size 1000
        \item Implements in-memory caching for keywords and ingredients
        \item Uses transaction management for atomicity
    \end{itemize}
    \item \texttt{CsvDataImporterCompare.java}: Implements baseline single insert method
    \begin{itemize}
        \item Uses \texttt{PreparedStatement.executeUpdate()} for each record
        \item No batching or caching optimizations
        \item Serves as baseline for performance comparison
    \end{itemize}
\end{itemize}

\textbf{3. Supporting Utilities (Task 3):}
\begin{itemize}
    \item \texttt{DataReader.java}: Handles CSV file reading and parsing
    \begin{itemize}
        \item Custom CSV parser supporting quoted fields with escaped quotes
        \item Parses comma-separated ID lists in various formats (\texttt{"""id1,id2"""}, \texttt{"id1,id2"}, \texttt{id1,id2})
        \item Data type conversion utilities (\texttt{parseLong}, \texttt{parseInteger}, \texttt{parseDouble}, \texttt{parseTimestamp})
    \end{itemize}
    \item \texttt{DataWriter.java}: Provides batch insert operations with configurable batch size (default: 1000)
    \item \texttt{DataQuery.java}: Provides query utilities for data validation and record counting
\end{itemize}

\textbf{4. Shared Components (common/):}
\begin{itemize}
    \item \texttt{ConnectionManager.java}: Manages database connections and transactions
    \begin{itemize}
        \item Handles connection lifecycle (creation, validation, closing)
        \item Provides transaction management (\texttt{commit}, \texttt{rollback}, \texttt{setAutoCommit})
        \item Used by both Task 3 and Task 4
    \end{itemize}
    \item \texttt{DatabaseConfig.java}: Loads database configuration from \texttt{db.properties}
    \begin{itemize}
        \item Supports file-based configuration or default values
        \item Provides connection parameters (host, port, database, user, password)
    \end{itemize}
    \item \texttt{db.properties}: Database connection configuration file
    \begin{itemize}
        \item Contains database connection settings
        \item Shared between Task 3 and Task 4
    \end{itemize}
\end{itemize}

\subsubsection{Execution Steps}

\textbf{Prerequisites:}
\begin{enumerate}
    \item Execute \texttt{database\_schema.sql} to create all 12 tables
    \item Configure \texttt{db.properties} with database connection settings:
    \begin{verbatim}
db.host=localhost
db.port=your database port
db.database=sustc_recipe_db
db.user=postgres
db.password=your_password
    \end{verbatim}
    \item Place CSV files in \texttt{final\_data/} directory
\end{enumerate}

\textbf{Compilation and Execution:}
\begin{verbatim}
# Compile all Java files
javac -encoding UTF-8 -cp ".;postgresql-42.2.5.jar" 
      src\main\task3\*.java src\main\common\*.java -d .

# Run batch insert method (optimized)
java -cp ".;postgresql-42.2.5.jar" main.Main

# Run single insert method (for comparison)
java -Xmx8g -cp ".;postgresql-42.2.5.jar" main.MainCompare
\end{verbatim}

\textbf{Note:} The project is organized into separate task directories (\texttt{src/main/task3/} for Task 3, \texttt{src/main/task4/} for Task 4, \texttt{src/main/common/} for shared components) for clear separation of concerns and modularity.

\subsection{Data Import Process}

In this section, we describe our data import process step by step: how we read data from CSV files, clean and prepare the data, and finally import it into the database. We also compare two different import methods and draw conclusions about optimal import strategies.

\subsubsection{Step 1: Reading Data from CSV Files}

The import process begins by reading data from three main CSV files located in the \texttt{final\_data/} directory:

\begin{itemize}
    \item \texttt{user.csv} - Contains user information (AuthorId, AuthorName, Gender, Age, FollowerUsers, etc.)
    \item \texttt{recipes.csv} - Contains recipe information (RecipeId, AuthorId, Name, CookTime, Keywords, RecipeIngredientParts, etc.)
    \item \texttt{reviews.csv} - Contains review information (ReviewId, RecipeId, AuthorId, Rating, Review, etc.)
\end{itemize}

We use a custom CSV parser (\texttt{DataReader.java}) that handles:
\begin{itemize}
    \item Quoted fields with escaped quotes (e.g., \texttt{"""value"""})
    \item Comma-separated ID lists in various formats
    \item UTF-8 encoding for proper character handling
\end{itemize}

The parser reads all CSV records into memory as \texttt{List<Map<String, String>>}, where each map represents a row with column names as keys.

\textbf{Raw Data Statistics:}
\begin{itemize}
    \item Users: 299,892 records read from \texttt{user.csv}
    \item Recipes: 928,283 records read from \texttt{recipes.csv}
    \item Reviews: 1,639,086 records read from \texttt{reviews.csv}
\end{itemize}

\subsubsection{Step 2: Data Cleaning and Preparation}

After reading the raw CSV data, we perform comprehensive data cleaning to ensure data quality before import:

\begin{enumerate}
    \item \textbf{Duplicate Detection:} We use \texttt{Set<Long> seenIds} to track primary keys and skip duplicate records. For example, if multiple rows have the same \texttt{authorid}, only the first occurrence is kept.
    
    \item \textbf{Null Value Handling:} Records with null primary keys or required fields are filtered out. For instance:
    \begin{itemize}
        \item Users without \texttt{AuthorId} or \texttt{AuthorName} are skipped
        \item Recipes without \texttt{RecipeId} are skipped
        \item Reviews without \texttt{ReviewId} or \texttt{Rating} are skipped
    \end{itemize}
    
    \item \textbf{Field Normalization:} We use \texttt{DataReader.normalizeField()} to:
    \begin{itemize}
        \item Trim leading and trailing whitespace
        \item Convert empty strings to null
        \item Handle special characters
    \end{itemize}
    
    \item \textbf{Data Type Conversion:} CSV string values are converted to appropriate database types:
    \begin{itemize}
        \item \texttt{parseLong()} for ID fields (with null handling for invalid values)
        \item \texttt{parseInteger()} for numeric fields (Age, Rating, etc.)
        \item \texttt{parseDouble()} for decimal fields (Calories, FatContent, etc.)
        \item \texttt{parseTimestamp()} for date fields (DatePublished, DateSubmitted, etc.)
    \end{itemize}
    
    \item \textbf{Complex Field Parsing:} We parse comma-separated ID lists (e.g., \texttt{FollowerUsers}, \texttt{FavoriteUsers}, \texttt{Likes}) with support for multiple formats:
    \begin{itemize}
        \item Triple-quoted format: \texttt{"""id1,id2,id3"""}
        \item Double-quoted format: \texttt{"id1,id2,id3"}
        \item Unquoted format: \texttt{id1,id2,id3}
    \end{itemize}
    
    \item \textbf{Conditional Data Insertion:} Some records are conditionally inserted:
    \begin{itemize}
        \item \texttt{nutrition} records are only inserted when \texttt{calories} is not null
        \item M2M relationships with null foreign keys are skipped
    \end{itemize}
\end{enumerate}

\textbf{Data Cleaning Results:}
\begin{itemize}
    \item Users: 299,892 valid records (100\% retention, no duplicates)
    \item Recipes: 522,517 valid records (56\% retention, 405,766 filtered due to duplicates/invalid data)
    \item Reviews: 1,401,983 valid records (85\% retention, 237,103 filtered due to invalid data)
\end{itemize}

After cleaning, we prepare data structures for all 12 tables:
\begin{itemize}
    \item Main entity tables: \texttt{users}, \texttt{recipes}, \texttt{reviews}, \texttt{nutrition}, \texttt{instructions}
    \item M2M base tables: \texttt{keywords}, \texttt{ingredients}
    \item M2M association tables: \texttt{recipe\_keywords}, \texttt{recipe\_ingredients}, \texttt{user\_favorite\_recipes}, \texttt{user\_liked\_reviews}, \texttt{user\_follows}
\end{itemize}

\subsubsection{Step 3: Database Import}

The import process follows a three-phase approach to ensure data integrity and optimize performance:

\textbf{Phase 1: M2M Base Tables}
\begin{itemize}
    \item Extract unique keywords and ingredients from recipe data
    \item Insert into \texttt{keywords} and \texttt{ingredients} tables
    \item Commit to ensure tables exist for subsequent operations
    \item Build in-memory caches (\texttt{keywordCache}, \texttt{ingredientCache}) for fast ID lookups
\end{itemize}

\textbf{Phase 2: Main Entity Tables}
\begin{itemize}
    \item Insert \texttt{users} table (299,892 records)
    \item Insert \texttt{recipes} table (522,517 records)
    \item Insert \texttt{reviews} table (1,401,983 records)
    \item Insert \texttt{nutrition} table (486,050 records)
    \item Insert \texttt{instructions} table (1,147,650 records)
\end{itemize}

\textbf{Phase 3: M2M Association Tables}
\begin{itemize}
    \item Use cached keyword/ingredient IDs to build \texttt{recipe\_keywords} (2,313,507 records)
    \item Use cached ingredient IDs to build \texttt{recipe\_ingredients} (3,711,215 records)
    \item Parse user relationship data to build \texttt{user\_favorite\_recipes} (1,251,900 records)
    \item Parse review like data to build \texttt{user\_liked\_reviews} (4,995,823 records)
    \item Parse follower data to build \texttt{user\_follows} (1,663,578 records)
\end{itemize}

All insertions use \texttt{ON CONFLICT DO NOTHING} to ensure idempotent imports (safe to re-run). The entire process is wrapped in a single transaction with \texttt{autoCommit=false}, committing on success or rolling back on error.

\subsubsection{Number of Data Entries}

Table~\ref{tab:record_counts} shows the number of records imported into each table.

\begin{table}[h]
\centering
\caption{Record Counts for Each Table}
\label{tab:record_counts}
\begin{tabular}{lrr}
\toprule
\textbf{Table Name} & \textbf{Number of Records} \\
\midrule
\texttt{users} & 299,892 \\
\texttt{recipes} & 522,517 \\
\texttt{reviews} & 1,401,983 \\
\texttt{nutrition} & 486,050 \\
\texttt{instructions} & 1,147,650 \\
\texttt{keywords} & 309 \\
\texttt{recipe\_keywords} & 2,313,507 \\
\texttt{ingredients} & 7,216 \\
\texttt{recipe\_ingredients} & 3,711,215 \\
\texttt{user\_favorite\_recipes} & 1,251,900 \\
\texttt{user\_liked\_reviews} & 4,995,823 \\
\texttt{user\_follows} & 1,663,578 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Total Records:} Approximately 18.1 million records across 12 tables.

\textbf{Import Statistics (Batch Insert Method):}
\begin{itemize}
    \item Total import time: $\sim$300 seconds (5.0 minutes, depending on system load)
    \item All tables successfully populated with data
    \item Data integrity verified through record count validation
\end{itemize}

\subsection{Import Method Comparison}

To understand the performance implications of different import strategies, we implemented and compared two methods:

\subsubsection{Method 1: Single Insert (Baseline)}

The first method (\texttt{CsvDataImporterCompare.java}) uses a straightforward approach:
\begin{itemize}
    \item For each record, create a \texttt{PreparedStatement} with the insert SQL
    \item Execute \texttt{executeUpdate()} immediately for each record
    \item Each insert is a separate database round-trip
\end{itemize}

\textbf{Implementation:}
\begin{verbatim}
for (Map<String, Object> row : data) {
    PreparedStatement pstmt = conn.prepareStatement(sql);
    // Set parameters...
    pstmt.executeUpdate();  // Single insert
}
\end{verbatim}

\subsubsection{Method 2: Batch Insert (Optimized)}

The second method (\texttt{CsvDataImporter.java}) uses batch processing:
\begin{itemize}
    \item Create \texttt{PreparedStatement} once before the loop
    \item Use \texttt{addBatch()} to add multiple records to a batch
    \item Execute \texttt{executeBatch()} when batch size reaches 1000
    \item Use \texttt{setAutoCommit(false)} to wrap all inserts in a single transaction
\end{itemize}

\textbf{Implementation:}
\begin{verbatim}
PreparedStatement pstmt = conn.prepareStatement(sql);
conn.setAutoCommit(false);
for (Map<String, Object> row : data) {
    // Set parameters...
    pstmt.addBatch();  // Add to batch
    if (batchCount % 1000 == 0) {
        pstmt.executeBatch();  // Execute batch
    }
}
pstmt.executeBatch();  // Execute remaining
conn.commit();
\end{verbatim}

\subsubsection{Key Optimizations in Batch Method}

\begin{enumerate}
    \item \textbf{Pre-compiled SQL:} \texttt{PreparedStatement} is created once and reused, eliminating SQL parsing overhead for each insert
    \item \textbf{Batch Processing:} Multiple inserts are combined into a single network round-trip, reducing network latency
    \item \textbf{In-memory Caching:} \texttt{keywordCache} and \texttt{ingredientCache} eliminate $\sim$6 million database lookups when building M2M tables
    \item \textbf{Transaction Management:} Single transaction reduces commit overhead
\end{enumerate}

Table~\ref{tab:import_performance} shows the performance comparison between these two methods.

\begin{table}[h]
\centering
\caption{Data Import Performance Comparison}
\label{tab:import_performance}
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Table} & \textbf{Records} & \textbf{Batch Insert} & \textbf{Single Insert} \\
\midrule
\texttt{users} & 299,892 & $\sim$15--20 s & $\sim$15--20 s \\
\texttt{recipes} & 522,517 & $\sim$25--30 s & $\sim$30--40 s \\
\texttt{reviews} & 1,401,983 & $\sim$40--50 s & $\sim$80--100 s \\
\texttt{nutrition} & 486,050 & $\sim$15--20 s & $\sim$25--30 s \\
\texttt{instructions} & 1,147,650 & $\sim$30--40 s & $\sim$60--70 s \\
\texttt{recipe\_keywords} & 2,313,507 & $\sim$60--80 s & $\sim$120--150 s \\
\texttt{recipe\_ingredients} & 3,711,215 & $\sim$80--100 s & $\sim$200--250 s \\
\texttt{user\_favorite\_recipes} & 1,251,900 & $\sim$30--40 s & $\sim$80--100 s \\
\texttt{user\_liked\_reviews} & 4,995,823 & $\sim$50--70 s & $\sim$350--400 s \\
\texttt{user\_follows} & 1,663,578 & $\sim$30--40 s & $\sim$100--120 s \\
\midrule
\textbf{Total} & \textbf{18.1M} & \textbf{$\sim$300 s (5.0 min)} & \textbf{1402.8 s (23.4 min)} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance Comparison Results}

Table~\ref{tab:import_performance} shows the detailed performance comparison between the two methods for importing 18.1 million records across 12 tables.

\textbf{Performance Improvement:} The batch insert method is approximately \textbf{4.7$\times$ faster} than the single insert method for the complete dataset (300 seconds vs 1402.8 seconds).

\subsubsection{Performance Analysis}

\textbf{Network Overhead Reduction:} The most significant factor is the reduction in network round-trips. For 18.1 million records:
\begin{itemize}
    \item Single insert method: 18.1 million network round-trips (one per record)
    \item Batch insert method: $\sim$18,100 network round-trips (one per batch of 1000)
    \item Reduction: approximately 1000$\times$ fewer network operations
\end{itemize}

\textbf{SQL Parsing Overhead:} 
\begin{itemize}
    \item Single insert: Each \texttt{executeUpdate()} requires SQL parsing and compilation
    \item Batch insert: SQL is parsed once per batch, reused for 1000 inserts
    \item Reduction: approximately 1000$\times$ fewer parsing operations
\end{itemize}

\textbf{Transaction Overhead:}
\begin{itemize}
    \item Single insert: Each insert may trigger implicit commit operations
    \item Batch insert: Single transaction for all inserts, one commit at the end
    \item Reduction: eliminates millions of commit operations
\end{itemize}

\textbf{In-Memory Caching:} Building \texttt{keywordCache} and \texttt{ingredientCache} eliminates $\sim$6 million database lookups when constructing M2M association tables. Without caching, each M2M relationship would require a database query to resolve keyword/ingredient IDs.

\subsection{Conclusion}

Based on our comprehensive comparison, we conclude that \textbf{batch insert is the optimal method for large-scale data imports}. The key findings are:

\begin{enumerate}
    \item \textbf{Batch processing is essential:} Combining multiple inserts into batches reduces network overhead by approximately 1000$\times$, which is the primary performance gain.
    
    \item \textbf{Pre-compiled statements matter:} Reusing \texttt{PreparedStatement} objects eliminates redundant SQL parsing, providing significant overhead reduction.
    
    \item \textbf{Transaction management is critical:} Using a single transaction with \texttt{autoCommit=false} eliminates millions of commit operations, further improving performance.
    
    \item \textbf{In-memory caching accelerates M2M tables:} Building caches for frequently accessed lookup data eliminates millions of database queries.
    
    \item \textbf{Practical impact:} For our dataset of 18.1 million records, batch insert completes in approximately 5 minutes, while single insert requires 23.4 minutes. This 4.7$\times$ speedup makes batch insert essential for production use.
\end{enumerate}

\textbf{Recommendation:} For any data import operation involving more than a few thousand records, batch insert with appropriate batch size (1000 is optimal for our use case) should be the standard approach. The single insert method, while simpler to implement and understand, is impractical for production use with large datasets.

\section{Task 4: Compare DBMS with File I/O}

\subsection{Test Environment and Data Organization}

\subsubsection{Test Environment}

Tests were conducted on the following platform:

\begin{itemize}
    \item \textbf{Operating System:} Windows 11
    \item \textbf{Database:} PostgreSQL 17.6
    \item \textbf{Programming Language:} Java JDK 17.0.4
    \item \textbf{CPU:} AMD Ryzen 9 7940HX
    \item \textbf{RAM:} 16GB
\end{itemize}

\subsubsection{Test Data Organization}

We use programmatic test data generation method implemented in \texttt{PerformanceTest.generateTestData()} to create test data with configurable sizes (5,000, 10,000, and 50,000 records). 

The generated test data is stored in two formats for comparison:
\begin{itemize}
    \item \textbf{Database:} PostgreSQL table \texttt{test\_performance} with schema (\texttt{id, name, value, category})
    \item \textbf{File I/O:} CSV files written to \texttt{test\_data/} directory
\end{itemize}

Both formats use identical data to ensure fair comparison. The data is generated simultaneously for both storage methods.

\subsubsection{Test Methods}

We compare the following methods for data operations:

\begin{enumerate}
    \item \textbf{Database Operations (DBMS):}
    \begin{itemize}
        \item Insert operations: Single-threaded vs multi-threaded (2, 4, 8 threads) batch inserts
        \item Query operations: Three types of SQL queries executed against PostgreSQL
    \end{itemize}
    
    \item \textbf{File I/O Operations:}
    \begin{itemize}
        \item Write operations: Writing test data to CSV files
        \item Query operations: Reading and parsing CSV files to perform searches
    \end{itemize}
    
    \item \textbf{In-Memory Operations:}
    \begin{itemize}
        \item Linear search: Using \texttt{ArrayList} for sequential search
        \item Indexed search: Using \texttt{BTreeIndex} for indexed search
    \end{itemize}
\end{enumerate}

\subsubsection{Test Queries}

Three types of SQL queries are tested to evaluate different database operations:

\begin{enumerate}
    \item \textbf{Point Query:} \texttt{SELECT * FROM test\_performance WHERE id = ?}
    \begin{itemize}
        \item Tests primary key index lookup performance
        \item Random \texttt{id} values generated for each query
    \end{itemize}
    
    \item \textbf{Range Query:} \texttt{SELECT * FROM test\_performance WHERE value >= ? AND value <= ?}
    \begin{itemize}
        \item Tests range scan performance
        \item Random range boundaries generated for each query
    \end{itemize}
    
    \item \textbf{Complex Aggregation Query:} \texttt{SELECT category, COUNT(*), AVG(value), MAX(value) FROM test\_performance WHERE category = ? AND value >= ? GROUP BY category}
    \begin{itemize}
        \item Tests aggregation and grouping performance
        \item Random \texttt{category} and \texttt{value} thresholds generated for each query
    \end{itemize}
\end{enumerate}

Each query type is executed 1000 times with randomly generated parameters, and the average execution time is calculated for statistical significance.

\subsubsection{Source Code Architecture}

The performance testing implementation is organized in \texttt{src/main/task4/}, with shared components in \texttt{src/main/common/}. The architecture consists of the following key components:

\textbf{1. PerformanceTest.java (src/main/task4/):}
This is the main orchestrator class that coordinates all performance tests. Its key responsibilities include:
\begin{itemize}
    \item \textbf{Test Data Generation:} \texttt{generateTestData(int size)} programmatically generates test data with configurable sizes (5k, 10k, 50k records). Each record contains \texttt{id}, \texttt{name}, \texttt{value}, and \texttt{category} fields with random values.
    \item \textbf{Insert Performance Testing:} 
    \begin{itemize}
        \item \texttt{testSingleThreadInsert()} - Tests single-threaded batch insert performance
        \item \texttt{testMultiThreadInsert()} - Tests multi-threaded batch insert performance with configurable thread counts (2, 4, 8 threads)
    \end{itemize}
    \item \textbf{Query Performance Testing:}
    \begin{itemize}
        \item \texttt{testDatabaseQuery()} - Tests point query performance using \texttt{WHERE id = ?}
        \item \texttt{testRangeQuery()} - Tests range query performance using \texttt{WHERE value >= ? AND value <= ?}
        \item \texttt{testComplexQuery()} - Tests complex aggregation query performance with \texttt{GROUP BY}
    \end{itemize}
    \item \textbf{Test Orchestration:}
    \begin{itemize}
        \item \texttt{runFullPerformanceTest()} - Executes comprehensive performance tests including DBMS vs File I/O comparison
        \item \texttt{runAdvancedPerformanceTest()} - Executes advanced tests with different data sizes, thread counts, and query types
    \end{itemize}
    \item \textbf{Performance Measurement:} Uses \texttt{System.nanoTime()} for high-precision timing measurements
\end{itemize}

\textbf{2. FileIOOperations.java (src/main/task4/):}
This class handles all File I/O operations for comparison with database operations. Its main methods include:
\begin{itemize}
    \item \texttt{writeDataToFile()} - Writes test data to CSV files in \texttt{test\_data/} directory. This simulates database insert operations and measures write performance.
    \item \texttt{searchFileForId()} - Performs point queries by reading and parsing CSV files line by line. Each query opens the file, reads sequentially, and searches for matching records. This represents the baseline File I/O query performance.
    \item \texttt{loadAllDataToMemory()} - Loads all CSV data into an \texttt{ArrayList} in memory for in-memory linear search comparison.
    \item \texttt{loadAllDataToBTree()} - Loads CSV data and builds a B-Tree index in memory for indexed search comparison.
\end{itemize}

\textbf{3. BTreeIndex.java (src/main/task4/):}
This class implements an in-memory B-Tree index data structure for comparison with database indexing. Key features:
\begin{itemize}
    \item \textbf{Data Structure:} Implements a B-Tree with configurable order (default order 3) for efficient key-value storage and retrieval
    \item \textbf{Operations:}
    \begin{itemize}
        \item \texttt{put(key, value)} - Inserts a key-value pair into the B-Tree
        \item \texttt{get(key)} - Searches for a value by key using O(log n) time complexity
        \item \texttt{rangeQuery(minKey, maxKey)} - Performs range queries to find all values within a key range
    \end{itemize}
    \item \textbf{Purpose:} Demonstrates the performance advantage of indexed data structures compared to linear search, providing a baseline for understanding database indexing benefits
\end{itemize}

\textbf{4. Shared Components (src/main/common/):}
These utility classes are shared between Task 3 and Task 4:
\begin{itemize}
    \item \texttt{ConnectionManager.java}: Manages database connections and transactions
    \begin{itemize}
        \item \texttt{getConnection()} - Obtains and manages PostgreSQL database connections
        \item \texttt{commit()}, \texttt{rollback()} - Handles transaction management for atomic operations
        \item \texttt{setAutoCommit()} - Controls transaction auto-commit behavior for batch operations
        \item \texttt{isConnectionValid()} - Validates connection status
    \end{itemize}
    \item \texttt{DatabaseConfig.java}: Loads database configuration
    \begin{itemize}
        \item \texttt{fromFile()} - Loads configuration from \texttt{db.properties}
        \item \texttt{defaultConfig()} - Provides default configuration values
        \item Manages connection parameters (host, port, database, user, password)
    \end{itemize}
    \item \texttt{db.properties}: Database connection configuration file
    \begin{itemize}
        \item Contains database connection settings (host, port, database, user, password)
        \item Shared between Task 3 and Task 4 for consistent database access
    \end{itemize}
\end{itemize}

\textbf{Compilation and Execution:}
\begin{verbatim}
# Compile all Java files
javac -encoding UTF-8 -cp ".;postgresql-42.2.5.jar" 
      src\main\task4\*.java src\main\common\*.java -d .

# Run full performance test
java -cp ".;postgresql-42.2.5.jar" main.Main

# Run advanced performance test
java -cp ".;postgresql-42.2.5.jar" main.Main advanced
\end{verbatim}

\textbf{Project Organization:} The source code is organized into task-specific directories (\texttt{src/main/task4/} for Task 4, \texttt{src/main/common/} for shared components) for clear separation of concerns and modularity.

\subsection{Performance Comparison Results}

\subsubsection{Insert Performance}

Table~\ref{tab:insert_performance} shows single-threaded vs multi-threaded insert performance.

\begin{table}[h]
\centering
\caption{Insert Performance Comparison}
\label{tab:insert_performance}
\begin{tabular}{lrrr}
\toprule
\textbf{Data Size} & \textbf{Single-threaded} & \textbf{Multi-threaded (4)} & \textbf{Ratio} \\
\midrule
5,000 records & 95 ms & 162 ms & 0.59$\times$ \\
10,000 records & 75 ms & 213 ms & 0.35$\times$ \\
50,000 records & 354 ms & 272 ms & 1.30$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Query Performance: DBMS vs File I/O}

Table~\ref{tab:query_performance} compares database queries with File I/O operations.

\begin{table}[h]
\centering
\caption{Query Performance: DBMS vs File I/O}
\label{tab:query_performance}
\begin{tabular}{lrrr}
\toprule
\textbf{Data Size} & \textbf{DBMS (ms)} & \textbf{File I/O (ms)} & \textbf{Speedup} \\
\midrule
5,000 records & 0.1570 & 0.4458 & 2.84$\times$ \\
10,000 records & 0.0721 & 0.5822 & 8.08$\times$ \\
50,000 records & 0.0717 & 2.3654 & 33.00$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{In-Memory Search Performance}

Table~\ref{tab:memory_performance} shows in-memory search performance (10,000 records, 1000 queries).

\begin{table}[h]
\centering
\caption{In-Memory Search Performance}
\label{tab:memory_performance}
\begin{tabular}{lr}
\toprule
\textbf{Method} & \textbf{Average Time (ms)} \\
\midrule
Database Query & 0.0721 \\
File I/O Query & 0.5822 \\
In-Memory (ArrayList) & 0.0220 \\
In-Memory (BTree) & 0.0026 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Thread Count Impact}

Table~\ref{tab:thread_performance} shows the impact of different thread counts (10,000 records).

\begin{table}[h]
\centering
\caption{Thread Count Impact on Insert Performance}
\label{tab:thread_performance}
\begin{tabular}{lrr}
\toprule
\textbf{Thread Count} & \textbf{Time (ms)} & \textbf{Ratio vs Single} \\
\midrule
1 (Single-threaded) & 75 & 1.00$\times$ \\
2 threads & 107 & 0.70$\times$ \\
4 threads & 102 & 0.74$\times$ \\
8 threads & 130 & 0.58$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Query Type Comparison}

Table~\ref{tab:query_types} compares different query types (10,000 records, 1000 queries).

\begin{table}[h]
\centering
\caption{Query Type Performance Comparison}
\label{tab:query_types}
\begin{tabular}{lr}
\toprule
\textbf{Query Type} & \textbf{Average Time (ms)} \\
\midrule
Point Query (Primary Key) & 0.0501 \\
Range Query & 1.0738 \\
Complex Aggregation Query & 1.0248 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance Analysis and Insights}

\subsubsection{Major Performance Differences}

\textbf{1. Database vs File I/O Performance:}

Database queries consistently outperform File I/O operations, with the performance advantage increasing dramatically as data size grows:
\begin{itemize}
    \item \textbf{5,000 records:} Database is 2.84$\times$ faster (0.157 ms vs 0.446 ms)
    \item \textbf{10,000 records:} Database is 8.08$\times$ faster (0.072 ms vs 0.582 ms)
    \item \textbf{50,000 records:} Database is 33.00$\times$ faster (0.072 ms vs 2.365 ms)
\end{itemize}

\textbf{Key Insight:} The performance gap widens significantly with data size, demonstrating the superior scalability of database systems. This is primarily due to:
\begin{itemize}
    \item \textbf{Indexing:} Primary key indexes enable O(log n) lookup time vs O(n) linear search in files
    \item \textbf{Query Optimization:} Database query planner optimizes execution paths
    \item \textbf{Buffering:} Database buffer pool caches frequently accessed data pages
    \item \textbf{File I/O Overhead:} File operations require full file reads and parsing for each query
\end{itemize}

\textbf{2. Query Type Performance:}

Different query types exhibit significantly different performance characteristics:
\begin{itemize}
    \item \textbf{Point Query (Primary Key):} 0.050 ms - Fastest, uses direct index lookup
    \item \textbf{Range Query:} 1.074 ms - Slower, requires index range scan
    \item \textbf{Complex Aggregation:} 1.025 ms - Moderate, requires grouping and aggregation operations
\end{itemize}

\textbf{Key Insight:} Point queries are approximately 21$\times$ faster than range queries because they use direct index access (O(log n)) rather than range scans. Complex queries with aggregations perform similarly to range queries, both requiring full or partial table scans with additional processing.

\textbf{3. In-Memory vs Database Performance:}

In-memory operations provide the highest performance but require loading all data into memory:
\begin{itemize}
    \item \textbf{Database Query:} 0.072 ms (10,000 records, 1000 queries)
    \item \textbf{File I/O Query:} 0.582 ms
    \item \textbf{In-Memory (ArrayList):} 0.022 ms - 3.3$\times$ faster than database
    \item \textbf{In-Memory (BTree):} 0.0026 ms - 27.7$\times$ faster than database
\end{itemize}

\textbf{Key Insight:} In-memory operations eliminate disk I/O overhead, providing dramatic performance improvements. BTree indexing provides 8.5$\times$ speedup over linear search (ArrayList), demonstrating the importance of appropriate data structures.

\textbf{4. Multi-threading Impact:}

Multi-threading shows mixed results depending on data size:
\begin{itemize}
    \item \textbf{Small datasets (5k, 10k):} Multi-threading is slower (0.35--0.59$\times$) due to thread overhead and synchronization costs
    \item \textbf{Large datasets (50k):} Multi-threading is faster (1.30$\times$) due to better resource utilization and parallel I/O operations
\end{itemize}

\textbf{Key Insight:} Thread overhead and synchronization costs dominate for small datasets, making multi-threading counterproductive. However, for larger datasets (50k+ records), parallel processing benefits become significant as I/O and computation can be effectively parallelized, resulting in performance improvements.

\subsubsection{Interesting Findings and Insights}

\begin{enumerate}
    \item \textbf{Database Performance Stability:} Database query performance remains remarkably stable across different data sizes (0.072--0.157 ms), demonstrating excellent scalability. This is due to efficient indexing and query optimization.
    
    \item \textbf{File I/O Performance Degradation:} File I/O performance degrades significantly with data size (0.446 ms to 2.365 ms), as larger files require more time to read and parse. This highlights the scalability limitations of file-based approaches.
    
    \item \textbf{Index Effectiveness:} The dramatic performance difference between point queries (0.050 ms) and range queries (1.074 ms) demonstrates the effectiveness of primary key indexes for exact lookups.
    
    \item \textbf{Memory vs Disk Trade-off:} In-memory operations (BTree: 0.0026 ms) are 27.7$\times$ faster than database queries, but require loading all data into memory. This represents a classic trade-off between speed and memory usage.
    
    \item \textbf{Scalability Advantage:} The performance advantage of databases over File I/O increases from 2.84$\times$ to 33.00$\times$ as data size grows, demonstrating superior scalability for large datasets.
    
    \item \textbf{Thread Overhead:} For small datasets (5k, 10k), multi-threading introduces overhead that outweighs benefits, resulting in slower performance (0.35--0.59$\times$). Only for larger datasets (50k+ records) does multi-threading provide performance gains (1.30$\times$).
\end{enumerate}

\subsubsection{Practical Implications}

Based on our comprehensive performance analysis, we draw the following practical conclusions:

\begin{enumerate}
    \item \textbf{For Production Systems:} Database systems are essential for applications requiring frequent queries, especially as data size grows. The 33.00$\times$ performance advantage for 50k records makes databases the clear choice.
    
    \item \textbf{For Small-Scale Applications:} File I/O may be acceptable for very small datasets (< 5k records) where simplicity is prioritized over performance, but even then, databases provide 3$\times$ better performance.
    
    \item \textbf{For High-Performance Requirements:} In-memory data structures (with appropriate indexing) provide the highest performance but require sufficient memory. This is suitable for frequently accessed, relatively static datasets.
    
    \item \textbf{For Large-Scale Systems:} Database systems demonstrate superior scalability, maintaining consistent performance as data size increases, while file-based approaches degrade significantly.
    
    \item \textbf{Query Type Selection:} Point queries should be preferred when possible, as they provide 21$\times$ better performance than range queries. Database schema design should prioritize primary key lookups for frequently accessed data.
\end{enumerate}

\subsubsection{Test Data Generation Method Benefits}

Our programmatic test data generation approach provides significant advantages for performance testing:

\begin{itemize}
    \item \textbf{Consistency:} Same data generation logic ensures identical test conditions for DBMS and File I/O comparisons, eliminating data-related variables
    \item \textbf{Scalability:} Easy to test different data sizes (5k, 10k, 50k, 100k+) without manual file preparation
    \item \textbf{Efficiency:} No need to store large test files; data is generated on-demand, reducing storage requirements
    \item \textbf{Reproducibility:} Fixed random seed enables consistent results across test runs, facilitating regression testing
    \item \textbf{Flexibility:} Can easily modify data distribution patterns (e.g., skewed distributions, different value ranges) for different test scenarios
    \item \textbf{Automation:} Fully automated generation enables comprehensive performance testing across different scales without manual intervention
\end{itemize}

This method ensures that both database and file I/O tests operate on identical datasets, providing fair and accurate performance comparisons that isolate the performance characteristics of the storage and query mechanisms themselves.

\end{document}
